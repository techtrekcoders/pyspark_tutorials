{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e92fd062-e26e-4d7f-9d93-0310f7a69989",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark SQL desc() Function: Sorting in Descending Order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4c89387-f5b0-4d99-b5ba-ee07141155d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduction to the `desc()` Function\n",
    "\n",
    "The `desc()` function in PySpark is used to sort a DataFrame in descending order based on one or more columns. It works similarly to the SQL `DESC` keyword used in `ORDER BY` statements and is often used when you want to rank or prioritize data, such as finding the top values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92d54a77-c58d-4ed6-b866-048a3e3e8320",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0161c299-11d3-4617-beff-15391a3cd2c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "DataFrame.orderBy(columnName.desc())\n",
    "```\n",
    "\n",
    "### Parameter:\n",
    "\n",
    "- **`columnName`**: The column you want to sort in descending order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afc63759-b1b9-4163-83e5-cb37e5268f78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Why Use `desc()`?\n",
    "\n",
    "- Sorting in descending order is useful when you want to display the largest, most recent, or highest values first.\n",
    "- It is particularly useful for ranking, leaderboards, financial data analysis (e.g., showing top sales), or highlighting the top performers in any dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5330cb59-8a7c-4043-a57a-7455ac636f93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd69bf94-e5fb-4d15-b342-44f3476585cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Sorting a Column in Descending Order\n",
    "\n",
    "**Scenario**: You have a DataFrame with sales data, and you want to sort it by `SALES` in descending order using `desc()`.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "255bbcee-2b0a-4282-96d4-4e393e87c5ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemB|  500|\n|ItemC|  400|\n|ItemA|  300|\n|ItemB|  200|\n|ItemA|  100|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (\"ItemA\", 100),\n",
    "    (\"ItemB\", 200),\n",
    "    (\"ItemA\", 300),\n",
    "    (\"ItemC\", 400),\n",
    "    (\"ItemB\", 500)\n",
    "], [\"ITEM\", \"SALES\"])\n",
    "\n",
    "# Sort by SALES in descending order using desc()\n",
    "df.orderBy(df.SALES.desc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1ec0bab-84b4-4930-a973-4b19fa3002e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Sorting Multiple Columns with Descending Order\n",
    "\n",
    "**Scenario**: You want to sort by `ITEM` in ascending order and then by `SALES` in descending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89e6ad69-11be-4892-80a5-b3028004d3c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sort by ITEM in ascending and SALES in descending order using desc()\n",
    "df.orderBy(\"ITEM\", df.SALES.desc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43e15d22-170a-44c9-b193-87cae41e671e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Using `desc()` with Multiple Aggregations\n",
    "\n",
    "**Scenario**: You want to group by `ITEM` and calculate the total sales, and then sort the result by `TOTAL_SALES` in descending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b3d9f3c-e0fc-41db-99de-12e199fbb86d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n| ITEM|TOTAL_SALES|\n+-----+-----------+\n|ItemB|        700|\n|ItemA|        400|\n|ItemC|        400|\n+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Group by ITEM, sum SALES, and sort by TOTAL_SALES in descending order\n",
    "df.groupBy(\"ITEM\").agg(sum(\"SALES\").alias(\"TOTAL_SALES\")).orderBy(desc(\"TOTAL_SALES\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8df655c7-80c3-4d11-8a90-689a92d686bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Using `desc()` with Column Expressions\n",
    "\n",
    "**Scenario**: You want to sort based on a calculated value, such as sorting by `SALES` after adding 10% to each value in descending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c4c39c-8ed8-49bb-859d-d9929d647a07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemB|  500|\n|ItemC|  400|\n|ItemA|  300|\n|ItemB|  200|\n|ItemA|  100|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Sort by SALES after increasing it by 10% in descending order\n",
    "df.orderBy(expr(\"SALES * 1.1\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b24d8cd3-3beb-4cf4-a42f-f809120473b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Handling Null Values with Descending Order\n",
    "\n",
    "**Scenario**: You want to sort by `SALES` in descending order while handling null values, placing them either first or last.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4384ae2-70b4-46a7-a739-1709947175f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemC|  400|\n|ItemA|  300|\n|ItemA|  100|\n|ItemB| null|\n|ItemB| null|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_with_nulls = spark.createDataFrame([\n",
    "    (\"ItemA\", 100),\n",
    "    (\"ItemB\", None),\n",
    "    (\"ItemA\", 300),\n",
    "    (\"ItemC\", 400),\n",
    "    (\"ItemB\", None)\n",
    "], [\"ITEM\", \"SALES\"])\n",
    "\n",
    "# Sort by SALES in descending order and place nulls last\n",
    "df_with_nulls.orderBy(df_with_nulls.SALES.desc_nulls_last()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92f4e77a-2086-4b57-b97e-c45a6595250f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 6. Sorting Strings in Descending Order\n",
    "\n",
    "**Scenario**: You want to sort a column of strings in descending alphabetical order using `desc()`.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb467c33-3c6e-4eaf-80b5-f230bbb6fd41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n|     FRUIT|QUANTITY|\n+----------+--------+\n|Elderberry|     500|\n|      Date|     400|\n|    Cherry|     300|\n|    Banana|     200|\n|     Apple|     100|\n+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df_items = spark.createDataFrame([\n",
    "    (\"Apple\", 100),\n",
    "    (\"Banana\", 200),\n",
    "    (\"Cherry\", 300),\n",
    "    (\"Date\", 400),\n",
    "    (\"Elderberry\", 500)\n",
    "], [\"FRUIT\", \"QUANTITY\"])\n",
    "\n",
    "# Sort by FRUIT in descending alphabetical order\n",
    "df_items.orderBy(df_items.FRUIT.desc()).show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "16_pyspark_desc_function_sorting_in_desc_order",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
