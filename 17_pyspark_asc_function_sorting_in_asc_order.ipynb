{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c637e167-a062-4420-89a9-28d65b7e6e1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark SQL asc() Function: Sorting in Ascending Order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f722bc3-6e95-4d62-804f-b44ce344f478",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduction to the `asc()` Function\n",
    "\n",
    "The `asc()` function in PySpark is used to sort a DataFrame in ascending order based on one or more columns. It is the opposite of the `desc()` function and works similarly to the SQL `ASC` keyword used in `ORDER BY` statements. Sorting in ascending order is common when you want to arrange data from smallest to largest, or alphabetically for string values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5872e7b8-3d25-4310-892e-e82b82eb8e16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b3d2256-167a-4d34-8fed-2047fbac1c20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "DataFrame.orderBy(columnName.asc())\n",
    "```\n",
    "\n",
    "### Parameter:\n",
    "\n",
    "- **`columnName`**: The column you want to sort in ascending order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce7cea4f-90e9-4061-8073-dea657bf3405",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Why Use `asc()`?\n",
    "\n",
    "- Sorting data in ascending order is useful for organizing data in an intuitive sequence, such as ranking from lowest to highest or sorting dates chronologically.\n",
    "- Itâ€™s particularly useful in scenarios where you want to present data in a natural order, such as sorting by price, age, date, or alphabetical order for text fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7372485d-0d3c-4487-9507-65bf541a7439",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd58aff1-b700-4d9f-9aa2-d183700d785a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Sorting a Column in Ascending Order\n",
    "\n",
    "**Scenario**: You have a DataFrame with sales data, and you want to sort it by `SALES` in ascending order using `asc()`.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c95a8ad-64db-4a18-92b5-94d310b6e3dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemE|  100|\n|ItemD|  200|\n|ItemB|  300|\n|ItemA|  500|\n|ItemC|  700|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (\"ItemA\", 500),\n",
    "    (\"ItemB\", 300),\n",
    "    (\"ItemC\", 700),\n",
    "    (\"ItemD\", 200),\n",
    "    (\"ItemE\", 100)\n",
    "], [\"ITEM\", \"SALES\"])\n",
    "\n",
    "# Sort by SALES in ascending order using asc()\n",
    "df.orderBy(df.SALES.asc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a3edbd6-59b9-4f12-9583-ee000de2967b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Sorting Multiple Columns with Ascending Order\n",
    "\n",
    "**Scenario**: You want to sort the DataFrame by `ITEM` and then by `SALES`, both in ascending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc19e69f-31f1-4931-b50d-ad1db238ee49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemA|  500|\n|ItemB|  300|\n|ItemC|  700|\n|ItemD|  200|\n|ItemE|  100|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort by ITEM and SALES in ascending order\n",
    "df.orderBy(df.ITEM.asc(), df.SALES.asc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2920b37b-ff3e-4299-b0f9-7aed50746b5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Using `asc()` with Multiple Aggregations\n",
    "\n",
    "**Scenario**: You want to group by `ITEM` and calculate the total sales, and then sort the result by `TOTAL_SALES` in ascending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83f760a5-f972-4a79-a284-3dbecb3fbe93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n| ITEM|TOTAL_SALES|\n+-----+-----------+\n|ItemE|        100|\n|ItemD|        200|\n|ItemB|        300|\n|ItemA|        500|\n|ItemC|        700|\n+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Group by ITEM, sum SALES, and sort by TOTAL_SALES in ascending order\n",
    "df.groupBy(\"ITEM\").agg(sum(\"SALES\").alias(\"TOTAL_SALES\")).orderBy(asc(\"TOTAL_SALES\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ee6bad0-b018-4230-8687-2e6607741670",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Using `asc()` with Column Expressions\n",
    "\n",
    "**Scenario**: You want to sort by a calculated value, such as sorting by `SALES` after adding 10% to each value in ascending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3405c2f6-4110-49f3-b3c8-7f5994393cdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemE|  100|\n|ItemD|  200|\n|ItemB|  300|\n|ItemA|  500|\n|ItemC|  700|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Sort by SALES after increasing it by 10% in ascending order\n",
    "df.orderBy(expr(\"SALES * 1.1\").asc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5d67146-57b0-450b-a9be-159065846dc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Handling Null Values with Ascending Order\n",
    "\n",
    "**Scenario**: You want to sort by `SALES` in ascending order while handling null values, placing them either first or last.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea701841-fc60-4205-a73c-4a965666f325",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemD|  200|\n|ItemA|  500|\n|ItemC|  700|\n|ItemE| null|\n|ItemB| null|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_with_nulls = spark.createDataFrame([\n",
    "    (\"ItemA\", 500),\n",
    "    (\"ItemB\", None),\n",
    "    (\"ItemC\", 700),\n",
    "    (\"ItemD\", 200),\n",
    "    (\"ItemE\", None)\n",
    "], [\"ITEM\", \"SALES\"])\n",
    "\n",
    "# Sort by SALES in ascending order and place nulls last\n",
    "df_with_nulls.orderBy(df_with_nulls.SALES.asc_nulls_last()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "369f6d9c-ddba-4c29-826d-d42e965622ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 6. Sorting Strings in Ascending Order\n",
    "\n",
    "**Scenario**: You want to sort a column of string values (e.g., `ITEM`) in ascending alphabetical order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5553426-ea21-4044-b52f-7981e950441e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemA|  500|\n|ItemB|  300|\n|ItemC|  700|\n|ItemD|  200|\n|ItemE|  100|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort by ITEM in ascending alphabetical order\n",
    "df.orderBy(df.ITEM.asc()).show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "17_pyspark_asc_function_sorting_in_asc_order",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
