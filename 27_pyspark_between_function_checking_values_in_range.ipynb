{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daf42c9d-32db-4967-afa6-457b67c7ec07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark SQL between() Function: Checking Values in a Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e247a93-721f-4708-b6e2-18ee7071069f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduction to the `between()` Function\n",
    "\n",
    "The `between()` function in PySpark is used to check if a columnâ€™s value falls within a specific range. It is similar to the SQL `BETWEEN` clause, where you define a lower and an upper bound, and the function returns true if the value is within that range. This function is commonly used for filtering numeric columns, dates, or other ordered data types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "442bc50f-64e6-4e76-bc42-d525ea3bba18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7a94c8a-cbbf-4bce-87d3-2c7a034bd9a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "DataFrame.filter(column.between(lower_bound, upper_bound))\n",
    "```\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **`column`**: The column whose values are checked.\n",
    "- **`lower_bound`**: The lower boundary of the range.\n",
    "- **`upper_bound`**: The upper boundary of the range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8309f892-3d69-4bb2-8ab3-e2a3089bd128",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Why Use `between()`?\n",
    "\n",
    "- It simplifies the process of filtering data based on a range. \n",
    "- Instead of writing multiple conditions (e.g., `>=` and `<=`), you can use `between()` for a more readable and concise approach.\n",
    "- It is especially useful for filtering numeric values, dates, or any data types that follow a natural order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87fcb0c9-300c-4599-afb4-c1fe6ee55431",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "239b7698-c7f1-4948-8690-1eee780b549b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Filtering Rows Based on a Numeric Range\n",
    "\n",
    "**Scenario**: You have a DataFrame with sales data, and you want to filter rows where sales fall between 300 and 600.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6319230d-6098-4c40-8bf5-ebe26b36b578",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n|product_name|sales|\n+------------+-----+\n|   Product A|  500|\n|   Product B|  300|\n|   Product D|  600|\n+------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"Product A\", 500),\n",
    "    (\"Product B\", 300),\n",
    "    (\"Product C\", 700),\n",
    "    (\"Product D\", 600)\n",
    "], [\"product_name\", \"sales\"])\n",
    "\n",
    "# Filter rows where sales are between 300 and 600\n",
    "df.filter(df.sales.between(300, 600)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac7156e3-8897-4396-bcac-5b46541bdd19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Using `between()` for Date Ranges\n",
    "\n",
    "**Scenario**: You have a DataFrame with transaction dates, and you want to filter rows where the date falls between two specific dates.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9793f108-baea-42db-b2c4-2bafb9d6f418",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n|  transaction|transaction_date|\n+-------------+----------------+\n|Transaction A|      2024-01-01|\n|Transaction B|      2024-06-15|\n+-------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_dates = spark.createDataFrame([\n",
    "    (\"Transaction A\", \"2024-01-01\"),\n",
    "    (\"Transaction B\", \"2024-06-15\"),\n",
    "    (\"Transaction C\", \"2024-10-01\")\n",
    "], [\"transaction\", \"transaction_date\"])\n",
    "\n",
    "# Filter rows where the transaction_date is between two dates\n",
    "df_dates.filter(F.col(\"transaction_date\").between(\"2024-01-01\", \"2024-09-30\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8a7a06a-62cc-4f8c-9d84-1d84e2b1ef65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Combining `between()` with Other Filters\n",
    "\n",
    "**Scenario**: You want to filter rows where the sales are between 300 and 600, and the product name starts with \"Product A\" or \"Product B.\"\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b818187-f73d-4565-af67-1ce93743bfe4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n|product_name|sales|\n+------------+-----+\n|   Product A|  500|\n+------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Combine between() with another condition\n",
    "df.filter(df.sales.between(300, 600) & df.product_name.startswith(\"Product A\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88da57b1-5c98-4ea3-9b70-ef8620002fc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Using `between()` for Conditional Aggregations\n",
    "\n",
    "**Scenario**: You want to calculate the total sales only for products whose sales are between 300 and 600.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c2318c-f1ba-417a-89d6-d0e0fabc5a8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|total_sales|\n+-----------+\n|       1400|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Aggregate sales for products between a specific sales range\n",
    "df.filter(df.sales.between(300, 600)).agg(sum(\"sales\").alias(\"total_sales\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bd49613-1b61-4b99-90ef-1b3dd8334c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Applying `between()` to String Values\n",
    "\n",
    "**Scenario**: You want to check if a string value (like product codes) falls alphabetically between two values.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc2e1216-88c8-4471-900d-c91bdd9df1a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n|product_name|product_code|\n+------------+------------+\n|   Product A|        A001|\n|   Product B|        B002|\n+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_codes = spark.createDataFrame([\n",
    "    (\"Product A\", \"A001\"),\n",
    "    (\"Product B\", \"B002\"),\n",
    "    (\"Product C\", \"C003\")\n",
    "], [\"product_name\", \"product_code\"])\n",
    "\n",
    "# Filter rows where product_code is between 'A001' and 'B999'\n",
    "df_codes.filter(df_codes.product_code.between(\"A001\", \"B999\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "decda2c8-eb3a-4eff-bc12-b10238835119",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 6. Handling Null Values with `between()`\n",
    "\n",
    "**Scenario**: You have a DataFrame with null values, and you want to apply `between()` while handling null values appropriately.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb64210c-fdb7-4947-ab2d-2e578d18b5e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n|product_name|sales|\n+------------+-----+\n|   Product A|  500|\n|   Product C|  600|\n+------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_with_nulls = spark.createDataFrame([\n",
    "    (\"Product A\", 500),\n",
    "    (\"Product B\", None),\n",
    "    (\"Product C\", 600)\n",
    "], [\"product_name\", \"sales\"])\n",
    "\n",
    "# Apply between() and handle null values\n",
    "df_with_nulls.filter(df_with_nulls.sales.between(300, 600)).show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "27_pyspark_between_function_checking_values_in_range",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
