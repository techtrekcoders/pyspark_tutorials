{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23680960-0257-4a19-8f76-87689d367e5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark SQL agg() Function: Mastering Aggregation Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4c969be-74b8-4fb5-8436-f1d38ae1ba5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduction to the `agg()` Function\n",
    "\n",
    "The `agg()` function in PySpark allows you to perform multiple aggregation operations on grouped data. It is commonly used in combination with the `groupBy()` function to compute summary statistics (like sum, average, count) across different columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a0e3b0d-7165-4520-92fd-58796701ff8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c42588da-85c9-4e12-9de6-3b1ff1bdf404",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "DataFrame.groupBy(*cols).agg(*expressions)\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **`cols`**: The columns to group by.\n",
    "- **`expressions`**: The aggregation functions to apply, such as `sum()`, `avg()`, `count()`, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84a62f5a-e4a4-4cc0-8942-d5d750164079",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Why Use `agg()`?\n",
    "\n",
    "- It allows you to apply multiple aggregations in a single statement.\n",
    "- You can customize your aggregate operations with aliases and apply different aggregate functions on different columns.\n",
    "- It simplifies writing complex analytical queries by chaining multiple aggregation expressions in one go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c074ec1b-8361-41e1-bc07-7c0d78165305",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Practical Examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec2acdd8-29e2-4799-a991-1fb215646a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Basic Aggregation with a Single Column\n",
    "\n",
    "**Scenario**: You want to calculate the total sales from a sales DataFrame.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "221a25c3-8b55-4ad1-84ea-62c6b7431de1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n| ITEM|TOTAL_SALES|\n+-----+-----------+\n|ItemA|         40|\n|ItemB|         60|\n|ItemC|         50|\n+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (\"ItemA\", 10),\n",
    "    (\"ItemB\", 20),\n",
    "    (\"ItemA\", 30),\n",
    "    (\"ItemB\", 40),\n",
    "    (\"ItemC\", 50)\n",
    "], [\"ITEM\", \"SALES\"])\n",
    "\n",
    "# Aggregate total SALES using sum()\n",
    "df.groupBy(\"ITEM\").agg(sum(\"SALES\").alias(\"TOTAL_SALES\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb3e87ff-976e-4d42-835d-01ce57e6e01b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Performing Multiple Aggregations\n",
    "\n",
    "**Scenario**: You want to calculate both the total and the average sales per item using `sum()` and `avg()`.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a33ccb47-1c45-45db-8370-c971884d8b02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------------+\n| ITEM|TOTAL_SALES|AVERAGE_SALES|\n+-----+-----------+-------------+\n|ItemA|         40|         20.0|\n|ItemB|         60|         30.0|\n|ItemC|         50|         50.0|\n+-----+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg\n",
    "\n",
    "# Apply multiple aggregations: sum and avg\n",
    "df.groupBy(\"ITEM\").agg(\n",
    "    sum(\"SALES\").alias(\"TOTAL_SALES\"),\n",
    "    avg(\"SALES\").alias(\"AVERAGE_SALES\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0a4c105-2312-49ed-ad84-3cd8c5f082ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Aggregating Multiple Columns\n",
    "\n",
    "**Scenario**: You have a DataFrame with both sales and profit data, and you want to calculate the total sales and total profit per item.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06cb0a46-faab-499f-9d01-63bfa5ce6359",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------------+\n| ITEM|TOTAL_SALES|TOTAL_PROFIT|\n+-----+-----------+------------+\n|ItemA|         40|           8|\n|ItemB|         60|          12|\n|ItemC|         50|          10|\n+-----+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_multi = spark.createDataFrame([\n",
    "    (\"ItemA\", 10, 2),\n",
    "    (\"ItemB\", 20, 4),\n",
    "    (\"ItemA\", 30, 6),\n",
    "    (\"ItemB\", 40, 8),\n",
    "    (\"ItemC\", 50, 10)\n",
    "], [\"ITEM\", \"SALES\", \"PROFIT\"])\n",
    "\n",
    "# Aggregate multiple columns: total SALES and total PROFIT\n",
    "df_multi.groupBy(\"ITEM\").agg(\n",
    "    sum(\"SALES\").alias(\"TOTAL_SALES\"),\n",
    "    sum(\"PROFIT\").alias(\"TOTAL_PROFIT\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db8d9dfa-e1e0-4227-a94e-08c64945aba8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Using Built-in Functions in `agg()`\n",
    "\n",
    "**Scenario**: You want to group by `ITEM` and calculate the maximum and minimum sales for each item.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c0568d-bfd7-4706-9096-189f9f943bee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+\n| ITEM|MAX_SALES|MIN_SALES|\n+-----+---------+---------+\n|ItemA|       30|       10|\n|ItemB|       40|       20|\n|ItemC|       50|       50|\n+-----+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min\n",
    "\n",
    "# Apply max() and min() aggregations\n",
    "df.groupBy(\"ITEM\").agg(\n",
    "    max(\"SALES\").alias(\"MAX_SALES\"),\n",
    "    min(\"SALES\").alias(\"MIN_SALES\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f6c40d9-41c2-4ac8-8bb1-975f52bd7437",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Aggregating Without Grouping (Global Aggregation)\n",
    "\n",
    "**Scenario**: You want to calculate the total sales across the entire DataFrame, without grouping by any specific column.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b648780-edc7-4a6a-891e-72ea00200b25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n|TOTAL_GLOBAL_SALES|\n+------------------+\n|               150|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Global aggregation: calculate total SALES without grouping\n",
    "df.agg(sum(\"SALES\").alias(\"TOTAL_GLOBAL_SALES\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ce280fd-5167-421a-805f-9e0e66e64e02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 6. Custom Aggregation Expressions\n",
    "\n",
    "**Scenario**: You want to use custom expressions to calculate more complex metrics, such as calculating the percentage of total sales for each item.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37989379-8322-45cc-b9fb-02be68c7b701",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n| ITEM|  SALES_PERCENTAGE|\n+-----+------------------+\n|ItemA|26.666666666666668|\n|ItemB|              40.0|\n|ItemC| 33.33333333333333|\n+-----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Calculate percentage of total sales\n",
    "total_sales = df.agg(sum(\"SALES\").alias(\"TOTAL\")).collect()[0][\"TOTAL\"]\n",
    "\n",
    "df.groupBy(\"ITEM\").agg(\n",
    "    (sum(\"SALES\") / total_sales * 100).alias(\"SALES_PERCENTAGE\")\n",
    ").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "8_pyspark_agg_function_mastering_agg_operations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
