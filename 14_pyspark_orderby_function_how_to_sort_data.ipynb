{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "115b05a6-7c8c-49b4-ad22-b7ccea114b32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark SQL orderBy() Function: How to Sort Data Easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efdbf10f-3763-4a2f-906a-711b48493c4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduction to the `orderBy()` Function\n",
    "\n",
    "The `orderBy()` function in PySpark is used to sort rows in a DataFrame based on one or more columns. It works similarly to the SQL `ORDER BY` clause, allowing you to sort data in ascending or descending order. It is one of the most commonly used functions for organizing data for better readability or further processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd97864b-0b61-4f0a-a24e-bd3c65df379d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cce92b37-f53f-4d82-85fa-fc66d8715303",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "DataFrame.orderBy(*cols, ascending=True)\n",
    "```\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **`cols`**: The columns you want to sort by.\n",
    "- **`ascending`**: A boolean value (`True` for ascending, `False` for descending). You can pass a list if sorting by multiple columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f037f6ff-9455-4308-8219-007fbfdc56ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Why Use `orderBy()`?\n",
    "\n",
    "- It helps organize data by sorting values in either ascending or descending order, making it easier to analyze trends, find maximum or minimum values, or present data more clearly.\n",
    "- Useful for reporting, ranking, and preparing data for further analysis or visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b012de6-ae2f-4d36-8d97-caad25f00493",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9ea919a-47f1-4e60-bb9c-e1c783d16262",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Sorting Data in Ascending Order\n",
    "\n",
    "**Scenario**: You have a DataFrame with sales data, and you want to sort it by `Sales` in ascending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883199f7-4b9f-4afd-b36e-014bca2e799b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemA|  100|\n|ItemB|  200|\n|ItemA|  300|\n|ItemC|  400|\n|ItemB|  500|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"ItemA\", 100),\n",
    "    (\"ItemB\", 200),\n",
    "    (\"ItemA\", 300),\n",
    "    (\"ItemC\", 400),\n",
    "    (\"ItemB\", 500)\n",
    "], [\"ITEM\", \"SALES\"])\n",
    "\n",
    "# Sort by SALES in ascending order\n",
    "df.orderBy(\"SALES\", ascending=True).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a45f9881-5d6d-4877-aa41-4f1edb6b16ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Sorting Data in Descending Order\n",
    "\n",
    "**Scenario**: You want to sort the sales data by `Sales` in descending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd2664de-070f-4758-9e72-0e0c3a47dbd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemB|  500|\n|ItemC|  400|\n|ItemA|  300|\n|ItemB|  200|\n|ItemA|  100|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort by SALES in descending order\n",
    "df.orderBy(\"SALES\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00acfb67-493a-458d-99ec-7ca37e6cc2a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Sorting by Multiple Columns\n",
    "\n",
    "**Scenario**: You want to sort the data first by `ITEM` and then by `SALES` in ascending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca64f86-02cb-4c20-81c3-d77b02007923",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemA|  100|\n|ItemA|  300|\n|ItemB|  200|\n|ItemB|  500|\n|ItemC|  400|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort by ITEM and then by SALES in ascending order\n",
    "df.orderBy(\"ITEM\", \"SALES\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "899a8b77-c12f-4db5-b7dc-c3c79d853951",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Sorting by Multiple Columns with Mixed Orders\n",
    "\n",
    "**Scenario**: You want to sort by `ITEM` in ascending order and `SALES` in descending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb7ebdc-1b3d-44bf-b80f-611f56bfac3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemA|  300|\n|ItemA|  100|\n|ItemB|  500|\n|ItemB|  200|\n|ItemC|  400|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort by ITEM in ascending and SALES in descending order\n",
    "df.orderBy([\"ITEM\", \"SALES\"], ascending=[True, False]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eedd5b95-f946-4433-9da2-239b92f82583",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Sorting by Column Expressions\n",
    "\n",
    "**Scenario**: You want to sort the data by the result of an expression, such as sorting based on sales values after adding 10% to each.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbb4f778-950d-4378-bba5-3803c218d7c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemA|  100|\n|ItemB|  200|\n|ItemA|  300|\n|ItemC|  400|\n|ItemB|  500|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Sort by SALES after increasing it by 10%\n",
    "df.orderBy(expr(\"SALES * 1.1\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81f27982-d05b-49ad-9569-d1e2a43f6126",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 6. Sorting Null Values\n",
    "\n",
    "**Scenario**: You want to sort the data while handling null values in the `SALES` column, placing nulls either first or last.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a6c350-7c6a-4c67-a69e-70c8df899a91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemB| null|\n|ItemB| null|\n|ItemA|  100|\n|ItemA|  300|\n|ItemC|  400|\n+-----+-----+\n\n+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemA|  100|\n|ItemA|  300|\n|ItemC|  400|\n|ItemB| null|\n|ItemB| null|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_with_nulls = spark.createDataFrame([\n",
    "    (\"ItemA\", 100),\n",
    "    (\"ItemB\", None),\n",
    "    (\"ItemA\", 300),\n",
    "    (\"ItemC\", 400),\n",
    "    (\"ItemB\", None)\n",
    "], [\"ITEM\", \"SALES\"])\n",
    "\n",
    "# Sort by SALES and place nulls first\n",
    "df_with_nulls.orderBy(df_with_nulls.SALES.asc_nulls_first()).show()\n",
    "\n",
    "# Sort by SALES and place nulls last\n",
    "df_with_nulls.orderBy(df_with_nulls.SALES.asc_nulls_last()).show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "14_pyspark_orderby_function_how_to_sort_data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
