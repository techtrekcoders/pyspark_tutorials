{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2378f3a-3a11-4744-ac8d-11b57cf269ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## PySpark SQL when() Function: Implementing Conditional Logic Easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9af1f7f-5376-42bc-98b3-089854a385fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduction to the `when()` Function\n",
    "\n",
    "The `when()` function in PySpark is used to implement conditional logic, similar to an if-else statement. It evaluates a condition, and if the condition is true, it returns a specified value. Otherwise, it returns a different value. This is especially useful when creating new columns based on existing values or applying conditional logic to rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "920fdf3d-0ead-41e2-9b9c-2fad13e821b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82d0b8ec-7556-4b49-b86d-22e987619601",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "DataFrame.select(when(condition, true_value).otherwise(false_value).alias(\"new_column\"))\n",
    "```\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **`condition`**: The logical condition to evaluate.\n",
    "- **`true_value`**: The value returned if the condition is true.\n",
    "- **`false_value`**: The value returned if the condition is false.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c48486-cf90-4794-bf7f-9e21dbb7e6d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Why Use `when()`?\n",
    "\n",
    "- The `when()` function is essential for data transformation tasks that require conditional logic, such as categorizing values, handling missing data, or applying complex conditions to determine the output of a new column.\n",
    "- It allows you to apply customized logic across rows, making it flexible for a variety of data cleaning and transformation operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bee2103b-64d8-4195-8b2d-ff0355a42158",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a67b613a-3765-4922-a6a5-2c16317b285c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Applying Simple Conditional Logic\n",
    "\n",
    "**Scenario**: You have a DataFrame with sales data, and you want to create a new column that labels sales as either \"High\" or \"Low\" based on a threshold value.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0048ebb2-0d73-4514-a15b-f4fc0745365d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+--------------+\n|product_name|sales|sales_category|\n+------------+-----+--------------+\n|   Product A|  500|          High|\n|   Product B|  200|           Low|\n|   Product C|  700|          High|\n+------------+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (\"Product A\", 500),\n",
    "    (\"Product B\", 200),\n",
    "    (\"Product C\", 700)\n",
    "], [\"product_name\", \"sales\"])\n",
    "\n",
    "# Apply conditional logic using when()\n",
    "df.select(\n",
    "    df.product_name,\n",
    "    df.sales,\n",
    "    when(df.sales > 300, \"High\").otherwise(\"Low\").alias(\"sales_category\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc0badf3-535e-43b6-8a40-13f9e04d2d94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Using Multiple Conditions\n",
    "\n",
    "**Scenario**: You want to categorize sales into \"High,\" \"Medium,\" or \"Low\" based on different thresholds.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b621f224-cdbc-44aa-ba05-d1df76113418",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+--------------+\n|product_name|sales|sales_category|\n+------------+-----+--------------+\n|   Product A|  500|        Medium|\n|   Product B|  200|           Low|\n|   Product C|  700|          High|\n+------------+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Apply multiple conditions using when()\n",
    "df.select(\n",
    "    df.product_name,\n",
    "    df.sales,\n",
    "    when(df.sales > 600, \"High\")\n",
    "    .when(df.sales > 300, \"Medium\")\n",
    "    .otherwise(\"Low\").alias(\"sales_category\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfdc28f4-92eb-4ce5-9829-2534fd774719",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Using `when()` for Conditional Column Creation\n",
    "\n",
    "**Scenario**: You have a column of ages, and you want to create a new column that labels each age group as \"Adult\" or \"Minor.\"\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d4c0f43-ead6-474c-960d-ecdf8b62811b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---------+\n|name|age|age_group|\n+----+---+---------+\n|John| 25|    Adult|\n|Jane| 17|    Minor|\n| Tom| 30|    Adult|\n+----+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df_age = spark.createDataFrame([\n",
    "    (\"John\", 25),\n",
    "    (\"Jane\", 17),\n",
    "    (\"Tom\", 30)\n",
    "], [\"name\", \"age\"])\n",
    "\n",
    "# Use when() to create a conditional column for age groups\n",
    "df_age.select(\n",
    "    df_age.name,\n",
    "    df_age.age,\n",
    "    when(df_age.age >= 18, \"Adult\").otherwise(\"Minor\").alias(\"age_group\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e786eb4c-cd7a-499c-88c5-21451fa7b815",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Combining `when()` with Other Functions\n",
    "\n",
    "**Scenario**: You want to apply conditional logic to sales data, where you mark sales as \"Valid\" if they are greater than 100 and replace null values with \"Unknown.\"\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bfb5c20-f29c-4e6d-8de8-8356ece74e36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n|product_name|sales_status|\n+------------+------------+\n|   Product A|       Valid|\n|   Product B|     Unknown|\n|   Product C|       Valid|\n+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_with_nulls = spark.createDataFrame([\n",
    "    (\"Product A\", 500),\n",
    "    (\"Product B\", None),\n",
    "    (\"Product C\", 700)\n",
    "], [\"product_name\", \"sales\"])\n",
    "\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Combine when() with other functions\n",
    "df_with_nulls.select(\n",
    "    df_with_nulls.product_name,\n",
    "    when(col(\"sales\").isNull(), lit(\"Unknown\"))\n",
    "    .when(col(\"sales\") > 100, \"Valid\")\n",
    "    .otherwise(\"Invalid\").alias(\"sales_status\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cf4f5ee-e267-4df6-a900-9e84460cc3cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Using `when()` for Conditional Updates\n",
    "\n",
    "**Scenario**: You want to update existing values based on certain conditions, such as marking all sales below 300 as \"Needs Improvement.\"\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "738e973a-e420-4dfe-8ae2-188723d555db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----------------+\n|product_name|sales|     sales_status|\n+------------+-----+-----------------+\n|   Product A|  500|             Good|\n|   Product B|  200|Needs Improvement|\n|   Product C|  700|             Good|\n+------------+-----+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Update sales_status conditionally using when()\n",
    "df.select(\n",
    "    df.product_name,\n",
    "    df.sales,\n",
    "    when(df.sales < 300, \"Needs Improvement\").otherwise(\"Good\").alias(\"sales_status\")\n",
    ").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "24_pyspark_when_function_implementing_conditional_logic",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
