{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b02012c-2724-4932-a282-b6e692a4f89c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark SQL groupBy() Function: Grouping Data Made Easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3c60878-b4ee-423f-87cc-5009791e562e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduction to the `groupBy()` Function\n",
    "\n",
    "The `groupBy()` function in PySpark is used to group the data based on one or more columns, similar to the `GROUP BY` clause in SQL. It is typically used when performing aggregate calculations (like sum, average, or count) on groups of data. This is crucial when you want to calculate summary statistics, identify trends, or gain insights into different segments of your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "675eed3b-7959-4297-a2e7-a05ce9bbb2c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f62523b-90ab-4483-96e5-bc3078a781e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "DataFrame.groupBy(*cols).agg(*expressions)\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **`cols`**: The column(s) to group by.\n",
    "- **`expressions`**: Aggregation expressions like `sum()`, `avg()`, `count()`, etc., that are applied to the grouped data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6fc9f0f-2c25-4b4a-865f-38e7634fb3b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## How It Works:\n",
    "\n",
    "- The `groupBy()` function creates groups based on the unique values in the specified columns.\n",
    "- After grouping, you typically use an aggregate function such as `sum()`, `count()`, `avg()`, etc., to compute metrics for each group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8d8568c-8380-4c65-8a93-2e538129dd5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a65eec03-7fad-4e39-92eb-1f55d9238805",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Grouping by a Single Column\n",
    "\n",
    "**Scenario**: You have a DataFrame with sales data, and you want to group the data by the `ITEM` column and calculate the total sales per item.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfab7e53-33d8-444d-b503-7205ac576cc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n| ITEM|sum(SALES)|\n+-----+----------+\n|ItemA|        40|\n|ItemB|        60|\n|ItemC|        50|\n+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"ItemA\", 10),\n",
    "    (\"ItemB\", 20),\n",
    "    (\"ItemA\", 30),\n",
    "    (\"ItemB\", 40),\n",
    "    (\"ItemC\", 50)\n",
    "], [\"ITEM\", \"SALES\"])\n",
    "\n",
    "# Group by ITEM and calculate total SALES\n",
    "df.groupBy(\"ITEM\").sum(\"SALES\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7909005-38dd-4b1e-9fac-2727b77dbb66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Grouping by Multiple Columns\n",
    "\n",
    "**Scenario**: You have sales data by `ITEM` and `REGION`, and you want to group by both columns to calculate the total sales per item in each region.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83146311-7c03-475c-8bb9-25bbc167c634",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------+\n| ITEM| REGION|sum(SALES)|\n+-----+-------+----------+\n|ItemA|Region1|        40|\n|ItemB|Region2|        60|\n|ItemC|Region3|        50|\n+-----+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_multi = spark.createDataFrame([\n",
    "    (\"ItemA\", \"Region1\", 10),\n",
    "    (\"ItemB\", \"Region2\", 20),\n",
    "    (\"ItemA\", \"Region1\", 30),\n",
    "    (\"ItemB\", \"Region2\", 40),\n",
    "    (\"ItemC\", \"Region3\", 50)\n",
    "], [\"ITEM\", \"REGION\", \"SALES\"])\n",
    "\n",
    "# Group by ITEM and REGION and calculate total SALES\n",
    "df_multi.groupBy(\"ITEM\", \"REGION\").sum(\"SALES\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb060d73-9523-4757-9876-24398d45060f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Using `agg()` with Multiple Aggregation Functions\n",
    "\n",
    "**Scenario**: You want to group by `ITEM` and calculate both the total sales and the average sales per item.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a205e79a-a39d-492c-b109-d87c5e7b0dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------------+\n| ITEM|TOTAL_SALES|AVERAGE_SALES|\n+-----+-----------+-------------+\n|ItemA|         40|         20.0|\n|ItemB|         60|         30.0|\n|ItemC|         50|         50.0|\n+-----+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg\n",
    "\n",
    "# Group by ITEM and apply multiple aggregation functions\n",
    "df.groupBy(\"ITEM\").agg(\n",
    "    sum(\"SALES\").alias(\"TOTAL_SALES\"),\n",
    "    avg(\"SALES\").alias(\"AVERAGE_SALES\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a92d4ed2-8f8d-4f4d-8f5d-c5ced104b017",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Grouping and Counting Rows\n",
    "\n",
    "**Scenario**: You want to group by `ITEM` and count how many times each item appears in the dataset.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa800f4c-e22e-468e-86e7-c9ffaf6be7a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|count|\n+-----+-----+\n|ItemA|    2|\n|ItemB|    2|\n|ItemC|    1|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Group by ITEM and count the occurrences of each item\n",
    "df.groupBy(\"ITEM\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc4df063-b95d-4c01-87ba-d6c6047c6b30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Filtering After Grouping (Using `having()` Equivalent)\n",
    "\n",
    "**Scenario**: You want to filter the groups based on the total sales, for example, only showing items with total sales greater than 50.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1748c9d8-78ed-448c-9a4c-b3871c407a83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n| ITEM|sum(SALES)|\n+-----+----------+\n|ItemB|        60|\n+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Group by ITEM, sum SALES, and filter groups where total SALES > 50\n",
    "df.groupBy(\"ITEM\").sum(\"SALES\").filter(\"sum(SALES) > 50\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "597a8e69-f936-403d-a8cf-f0dba8331de7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 6. Grouping and Sorting\n",
    "\n",
    "**Scenario**: You want to group by `ITEM`, calculate the total sales, and then sort the results by total sales in descending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e8f40d-9207-4567-a048-e78a1858780e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n| ITEM|sum(SALES)|\n+-----+----------+\n|ItemB|        60|\n|ItemC|        50|\n|ItemA|        40|\n+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Group by ITEM, sum SALES, and sort the result in descending order of total SALES\n",
    "df.groupBy(\"ITEM\").sum(\"SALES\").orderBy(\"sum(SALES)\", ascending=False).show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "7_pyspark_group_function_grouping_data_made_easy",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
