{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cacfd1e3-e5e2-4dd6-aced-5aa94599dab6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark SQL case when: Handling SQL-Style Conditional Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61d212b6-72ef-45fc-9a29-4455c5b1e321",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduction to the `case when` Expression\n",
    "\n",
    "The `case when` expression in PySpark allows you to apply SQL-style conditional logic. It works similarly to the `CASE WHEN` statement in SQL, where you define multiple conditions, and for each condition that evaluates as true, a specific value is returned. This is helpful for creating conditional logic with multiple outcomes, all in a single expression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c396a85-423c-4ed7-95c1-3b22ac9e88fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b28270e-59e3-44d5-94e4-873f54fc2dc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "DataFrame.select(expr(\"CASE WHEN condition THEN true_value ELSE false_value END\").alias(\"new_column\"))\n",
    "```\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **`condition`**: The logical condition to evaluate.\n",
    "- **`true_value`**: The value returned if the condition is true.\n",
    "- **`false_value`**: The value returned if the condition is false.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d157554c-b184-4bf0-acee-e5321b5f1114",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Why Use `case when`?\n",
    "\n",
    "- It simplifies complex conditional logic and is familiar to those who use SQL. Instead of writing multiple `when()` functions, you can use a more compact SQL-style syntax to define conditions and outcomes.\n",
    "- It is particularly useful for data transformations, where multiple conditions need to be checked and different values assigned based on those conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a53bb09-259f-4ffd-ad67-b8ec0ea51bb9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74df2fa9-95ab-4b40-b561-18e64682b822",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Applying a Single `case when` Condition\n",
    "\n",
    "**Scenario**: You have a DataFrame with sales data, and you want to label sales as \"High\" if they are greater than 500, and \"Low\" otherwise.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b01a42ae-6d66-4e8d-b53a-236914898d9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n|sales_category|\n+--------------+\n|          High|\n|           Low|\n|          High|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (\"Product A\", 600),\n",
    "    (\"Product B\", 300),\n",
    "    (\"Product C\", 800)\n",
    "], [\"product_name\", \"sales\"])\n",
    "\n",
    "# Apply a single case when condition\n",
    "df.select(expr(\n",
    "    \"CASE WHEN sales > 500 THEN 'High' ELSE 'Low' END\"\n",
    ").alias(\"sales_category\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51dd0770-e85a-4bcb-9cc9-98f1f96d7622",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Using Multiple `case when` Conditions\n",
    "\n",
    "**Scenario**: You want to categorize sales as \"High,\" \"Medium,\" or \"Low\" based on different thresholds.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d59ff3d-9598-45d2-b5ad-852c5d31d3c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n|sales_category|\n+--------------+\n|        Medium|\n|           Low|\n|          High|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Apply multiple case when conditions\n",
    "df.select(expr(\n",
    "    \"CASE WHEN sales > 700 THEN 'High' \" +\n",
    "    \"WHEN sales > 400 THEN 'Medium' \" +\n",
    "    \"ELSE 'Low' END\"\n",
    ").alias(\"sales_category\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30acefa5-83d0-4344-b16b-c2efe197534e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Creating a New Column Based on `case when`\n",
    "\n",
    "**Scenario**: You want to add a new column to a DataFrame of ages, where ages 18 and above are labeled \"Adult\" and below 18 as \"Minor.\"\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceccb59a-1fd2-4ba8-9a4c-380faccc50c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|age_group|\n+---------+\n|    Adult|\n|    Minor|\n|    Adult|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df_age = spark.createDataFrame([\n",
    "    (\"John\", 25),\n",
    "    (\"Jane\", 16),\n",
    "    (\"Tom\", 18)\n",
    "], [\"name\", \"age\"])\n",
    "\n",
    "# Create a new column based on age groups using case when\n",
    "df_age.select(expr(\n",
    "    \"CASE WHEN age >= 18 THEN 'Adult' ELSE 'Minor' END\"\n",
    ").alias(\"age_group\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2236ffd6-92f4-4ad6-aef6-1cc236614380",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Combining `case when` with Other Functions\n",
    "\n",
    "**Scenario**: You want to check if sales are null, and return \"Unknown\" if null, \"Valid\" if sales are greater than 100, and \"Invalid\" otherwise.\n",
    "\n",
    "**Code Example**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "001af848-db47-486a-a87c-17077fb3f5d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|sales_status|\n+------------+\n|       Valid|\n|     Unknown|\n|     Invalid|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_with_nulls = spark.createDataFrame([\n",
    "    (\"Product A\", 500),\n",
    "    (\"Product B\", None),\n",
    "    (\"Product C\", 50)\n",
    "], [\"product_name\", \"sales\"])\n",
    "\n",
    "# Combine case when with null checks\n",
    "df_with_nulls.select(expr(\n",
    "    \"CASE WHEN sales IS NULL THEN 'Unknown' \" +\n",
    "    \"WHEN sales > 100 THEN 'Valid' \" +\n",
    "    \"ELSE 'Invalid' END\"\n",
    ").alias(\"sales_status\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2442c3e8-3741-4459-b9bf-bca865c0c0a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Nested `case when` Conditions\n",
    "\n",
    "**Scenario**: You want to apply nested conditional logic, such as categorizing customers based on both their age and location.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ffb88bf-cdf4-4523-b5c5-921f134bb01e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|         status|\n+---------------+\n|   Adult in USA|\n|Minor in Canada|\n|   Adult in USA|\n+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_customers = spark.createDataFrame([\n",
    "    (\"John\", 25, \"USA\"),\n",
    "    (\"Jane\", 17, \"Canada\"),\n",
    "    (\"Tom\", 30, \"USA\")\n",
    "], [\"name\", \"age\", \"country\"])\n",
    "\n",
    "# Apply nested case when logic\n",
    "df_customers.select(expr(\n",
    "    \"CASE WHEN country = 'USA' THEN \" +\n",
    "    \"  CASE WHEN age >= 18 THEN 'Adult in USA' ELSE 'Minor in USA' END \" +\n",
    "    \"WHEN country = 'Canada' THEN \" +\n",
    "    \"  CASE WHEN age >= 18 THEN 'Adult in Canada' ELSE 'Minor in Canada' END \" +\n",
    "    \"ELSE 'Other' END\"\n",
    ").alias(\"status\")).show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "25_pyspark_case_function_handling_sql_style_logic",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
