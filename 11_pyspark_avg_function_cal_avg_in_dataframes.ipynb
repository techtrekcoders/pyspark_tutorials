{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35fbf3b1-c00d-4228-9393-978ddce9dd18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark SQL avg() Function: Calculating Averages in DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "998b5faa-fe08-4b0c-850b-2a084a2aa280",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduction to the `avg()` Function\n",
    "\n",
    "The `avg()` function in PySpark is used to calculate the average (mean) value of numeric columns. It works similarly to the SQL `AVG()` function, providing a quick way to compute the mean for any numeric data column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "876bc160-8e36-4200-94da-a6c234c33c72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df014cd0-6ec9-4c33-8b45-3caf97d5d770",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "DataFrame.groupBy(*cols).agg(avg(\"columnName\"))\n",
    "```\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **`cols`**: The column(s) to group by (if necessary).\n",
    "- **`avg(\"columnName\")`**: Applies the average function to the specified column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b574ee46-e69c-4336-80a1-9e4d29ce4beb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Why Use `avg()`?\n",
    "\n",
    "- Itâ€™s a common aggregation operation used in financial analysis, data profiling, and summary reporting to determine the mean value of datasets.\n",
    "- It works well for understanding central tendencies in numeric data and comparing groups of data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0bc5115-2320-471c-891f-608c797b87f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a470078c-0e7a-4155-ae7a-04e22cdafaad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Calculating the Average of a Single Column\n",
    "\n",
    "**Scenario**: You have a DataFrame with sales data, and you want to calculate the average sales across all items.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f30620d-f40d-45d2-8ab1-d214f3b37065",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|AVERAGE_SALES|\n+-------------+\n|        300.0|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (\"ItemA\", 100),\n",
    "    (\"ItemB\", 200),\n",
    "    (\"ItemA\", 300),\n",
    "    (\"ItemC\", 400),\n",
    "    (\"ItemB\", 500)\n",
    "], [\"ITEM\", \"SALES\"])\n",
    "\n",
    "# Calculate the average of the SALES column\n",
    "df.agg(avg(\"SALES\").alias(\"AVERAGE_SALES\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a598e97-8cf0-4a85-86f8-18bf27df6ffb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Grouping and Calculating Averages\n",
    "\n",
    "**Scenario**: You want to group by `ITEM` and calculate the average sales for each item.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5762f917-f758-4a94-b851-7e48f3b6f560",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n| ITEM|AVERAGE_SALES|\n+-----+-------------+\n|ItemA|        200.0|\n|ItemB|        350.0|\n|ItemC|        400.0|\n+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Group by ITEM and calculate the average SALES for each item\n",
    "df.groupBy(\"ITEM\").agg(avg(\"SALES\").alias(\"AVERAGE_SALES\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e6abee7-8f1b-46c1-8666-7c384faf6f6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Calculating Averages for Multiple Columns\n",
    "\n",
    "**Scenario**: You have sales and profit data, and you want to calculate the average sales and the average profit across the entire DataFrame.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "756cd81f-d0e9-4c84-bfac-8c9395173a9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n|AVERAGE_SALES|AVERAGE_PROFIT|\n+-------------+--------------+\n|        300.0|          30.0|\n+-------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_multi = spark.createDataFrame([\n",
    "    (\"ItemA\", 100, 10),\n",
    "    (\"ItemB\", 200, 20),\n",
    "    (\"ItemA\", 300, 30),\n",
    "    (\"ItemC\", 400, 40),\n",
    "    (\"ItemB\", 500, 50)\n",
    "], [\"ITEM\", \"SALES\", \"PROFIT\"])\n",
    "\n",
    "# Calculate the average for both SALES and PROFIT columns\n",
    "df_multi.agg(\n",
    "    avg(\"SALES\").alias(\"AVERAGE_SALES\"),\n",
    "    avg(\"PROFIT\").alias(\"AVERAGE_PROFIT\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6e93a8e-0492-455b-9178-f449b2498a4d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Grouping and Calculating Multiple Averages\n",
    "\n",
    "**Scenario**: You want to group by `ITEM` and calculate both the average sales and average profit for each item.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "080712a4-89c8-4608-8c0c-8b3212de3d61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------+\n| ITEM|AVERAGE_SALES|AVERAGE_PROFIT|\n+-----+-------------+--------------+\n|ItemA|        200.0|          20.0|\n|ItemB|        350.0|          35.0|\n|ItemC|        400.0|          40.0|\n+-----+-------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Group by ITEM and calculate both average SALES and average PROFIT\n",
    "df_multi.groupBy(\"ITEM\").agg(\n",
    "    avg(\"SALES\").alias(\"AVERAGE_SALES\"),\n",
    "    avg(\"PROFIT\").alias(\"AVERAGE_PROFIT\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dacd99a6-3d71-444b-9abe-9df5e16131b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Using `avg()` with a Conditional Expression\n",
    "\n",
    "**Scenario**: You want to calculate the average sales only for rows where sales exceed 200.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fa0884b-d066-486c-b219-a3bf59274752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n|AVERAGE_SALES_OVER_200|\n+----------------------+\n|                 400.0|\n+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Calculate the average sales where SALES > 200\n",
    "df.agg(avg(when(df.SALES > 200, df.SALES)).alias(\"AVERAGE_SALES_OVER_200\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f52a7358-32f2-4285-ad72-2ac898a564e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 6. Calculating a Global Average\n",
    "\n",
    "**Scenario**: You want to calculate the average sales across the entire dataset, without any grouping.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d978c001-1f0d-493f-9a53-ed8790d9a4a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|GLOBAL_AVERAGE_SALES|\n+--------------------+\n|               300.0|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Global average of SALES\n",
    "df.agg(avg(\"SALES\").alias(\"GLOBAL_AVERAGE_SALES\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc439173-d822-48b8-ba79-79787ea04ee9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "11_pyspark_avg_function_cal_avg_in_dataframes",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
