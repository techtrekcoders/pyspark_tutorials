{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f18fb544-45bb-44ed-a15a-0f28ce80b15f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## PySpark SQL regexp_replace() Function: Replacing Substrings with Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc3e95a3-2d8c-4f27-89cb-bdfc983b64d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduction to the `regexp_replace()` Function\n",
    "\n",
    "The `regexp_replace()` function in PySpark is used to replace parts of a string column based on a regular expression (regex). It allows you to search for a pattern in a string and replace it with another substring. This is particularly useful for cleaning or transforming text data where patterns like special characters, spaces, or certain words need to be replaced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3324ffd9-3d6f-42d6-b908-76dc9c1c6e40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8be4be93-74f8-4e44-82d0-ec60c09a2845",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "DataFrame.select(regexp_replace(column, pattern, replacement).alias(\"new_column\"))\n",
    "```\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **`column`**: The column containing the string to modify.\n",
    "- **`pattern`**: The regex pattern to search for.\n",
    "- **`replacement`**: The string to replace the matched pattern with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1aa13205-ab06-42f8-86ad-f2d68d630cca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Why Use `regexp_replace()`?\n",
    "\n",
    "- Itâ€™s a powerful tool for cleaning and transforming data where you need to identify patterns in strings, such as removing special characters, correcting formatting, or replacing certain words.\n",
    "- Regular expressions allow you to define complex patterns that can match multiple variations, making this function flexible for a wide range of tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33cb7172-f66e-4e66-af73-da466df525a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07ac7108-27e5-4134-a502-e35ac1180b2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Replacing All Occurrences of a Substring\n",
    "\n",
    "**Scenario**: You have a DataFrame with product codes, and you want to replace all dashes (`-`) with spaces.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4d654f2-a1e5-4dfa-bad7-60d70741a9f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|cleaned_product_code|\n+--------------------+\n|         ABC 123 XYZ|\n|         DEF 456 UVW|\n|         GHI 789 RST|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (\"ABC-123-XYZ\",),\n",
    "    (\"DEF-456-UVW\",),\n",
    "    (\"GHI-789-RST\",)\n",
    "], [\"product_code\"])\n",
    "\n",
    "# Replace dashes with spaces using regexp_replace()\n",
    "df.select(regexp_replace(df.product_code, \"-\", \" \").alias(\"cleaned_product_code\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a865914-3f91-4f8b-97a4-ad8c8ee90540",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Removing Special Characters\n",
    "\n",
    "**Scenario**: You want to clean up a column by removing all non-alphanumeric characters (such as punctuation).\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d48d80ad-19e1-4040-97a4-4a75068fbf05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|cleaned_text|\n+------------+\n|  Product123|\n|     Price50|\n|     CodeABC|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Remove all non-alphanumeric characters\n",
    "df_special = spark.createDataFrame([\n",
    "    (\"Product@123!\",),\n",
    "    (\"Price$50%\",),\n",
    "    (\"Code#ABC\",)\n",
    "], [\"text\"])\n",
    "\n",
    "df_special.select(regexp_replace(df_special.text, \"[^a-zA-Z0-9]\", \"\").alias(\"cleaned_text\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdb5e21a-817b-4861-b39f-e2361ca90749",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Replacing Patterns in Date Strings\n",
    "\n",
    "**Scenario**: You have dates in different formats (e.g., `2023/10/12`, `2023-10-12`), and you want to standardize them by replacing slashes (`/`) and dashes (`-`) with dots (`.`).\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df179565-39b6-4435-a702-8f688e291aa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|standardized_date|\n+-----------------+\n|       2023.10.12|\n|       2023.10.12|\n|       2023.01.01|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_dates = spark.createDataFrame([\n",
    "    (\"2023/10/12\",),\n",
    "    (\"2023-10-12\",),\n",
    "    (\"2023/01/01\",)\n",
    "], [\"date\"])\n",
    "\n",
    "# Standardize date format by replacing slashes and dashes with dots\n",
    "df_dates.select(\n",
    "    regexp_replace(df_dates.date, \"[-/]\", \".\").alias(\"standardized_date\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49cc0332-aa27-4694-8cae-56bc84a6ca40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Replacing Multiple Patterns in One Step\n",
    "\n",
    "**Scenario**: You want to replace multiple patterns in a single transformation, such as replacing commas (`,`) and semicolons (`;`) with spaces.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbdaa8d0-ee5c-431f-b06c-0e3199141070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n|      cleaned_text|\n+------------------+\n|     Hello  world |\n|        Hi  there |\n|Goodbye  everyone |\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_text = spark.createDataFrame([\n",
    "    (\"Hello, world;\",),\n",
    "    (\"Hi, there;\",),\n",
    "    (\"Goodbye, everyone;\",)\n",
    "], [\"text\"])\n",
    "\n",
    "# Replace both commas and semicolons with spaces\n",
    "df_text.select(\n",
    "    regexp_replace(df_text.text, \"[,;]\", \" \").alias(\"cleaned_text\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5781930f-d11e-414f-85c2-14e31ff7f954",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Using `regexp_replace()` with Complex Regex\n",
    "\n",
    "**Scenario**: You want to replace all numeric values in a column with the word \"NUMBER\".\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75862dcf-aa3d-4b2b-a43d-d7bc1d3a0c31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|cleaned_description|\n+-------------------+\n|        Item NUMBER|\n|     Product NUMBER|\n|       Order NUMBER|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_numbers = spark.createDataFrame([\n",
    "    (\"Item 123\",),\n",
    "    (\"Product 456\",),\n",
    "    (\"Order 789\",)\n",
    "], [\"description\"])\n",
    "\n",
    "# Replace all numeric values with the word \"NUMBER\"\n",
    "df_numbers.select(\n",
    "    regexp_replace(df_numbers.description, \"\\\\d+\", \"NUMBER\").alias(\"cleaned_description\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5147e87c-df58-456d-8b2f-b81ad50602f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 6. Handling Null Values with `regexp_replace()`\n",
    "\n",
    "**Scenario**: You want to use `regexp_replace()` on a column that contains null values and ensure that nulls are not affected by the replacement process.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "699200c2-09d4-4cc1-9d2f-fad79457e5ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|cleaned_product_code|\n+--------------------+\n|             ABC 123|\n|                null|\n|             XYZ 789|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_with_nulls = spark.createDataFrame([\n",
    "    (\"ABC-123\",),\n",
    "    (None,),\n",
    "    (\"XYZ-789\",)\n",
    "], [\"product_code\"])\n",
    "\n",
    "# Handle null values safely while replacing dashes\n",
    "df_with_nulls.select(\n",
    "    regexp_replace(df_with_nulls.product_code, \"-\", \" \").alias(\"cleaned_product_code\")\n",
    ").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "22_pyspark_reg_replace_function_replacing_substring_with_regex",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
