{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80abf27a-396f-408b-8f44-5ddabaa23e15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark SQL sort() Function: Sorting DataFrames Made Easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbb05eba-f900-474f-8e0e-dadeb2bc4c91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduction to the `sort()` Function\n",
    "\n",
    "The `sort()` function in PySpark is used to sort a DataFrame based on one or more columns. It is essentially the same as `orderBy()`, but with slightly different syntax. Both functions are used interchangeably for sorting DataFrames in ascending or descending order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a62fa0d-6725-443a-848f-34e007d446eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f09e0e2-f3aa-468a-8f8b-b0be89662de1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "DataFrame.sort(*cols, ascending=True)\n",
    "```\n",
    "### Parameters:\n",
    "\n",
    "- **`cols`**: The column(s) you want to sort by.\n",
    "- **`ascending`**: A boolean value (`True` for ascending, `False` for descending). You can pass a list if sorting by multiple columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "519a496e-43e2-4973-84b9-a85532fde5a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Why Use `sort()`?\n",
    "\n",
    "- Like `orderBy()`, the `sort()` function is used to organize data for easy analysis or presentation. Itâ€™s particularly useful when you want to rank, prioritize, or visualize data in a specific order.\n",
    "- Both `sort()` and `orderBy()` function similarly, but `sort()` is more commonly used when working with a small number of columns or straightforward sorting needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8362120-2526-4813-ace1-f00e7c4712c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5d0f949-9dc5-4a56-b424-f6728918ed24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Sorting Data in Ascending Order\n",
    "\n",
    "**Scenario**: You have a DataFrame with sales data, and you want to sort it by `Sales` in ascending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a19c90-97b1-4445-9b96-2c09a419c095",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemA|  100|\n|ItemB|  200|\n|ItemA|  300|\n|ItemC|  400|\n|ItemB|  500|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"ItemA\", 100),\n",
    "    (\"ItemB\", 200),\n",
    "    (\"ItemA\", 300),\n",
    "    (\"ItemC\", 400),\n",
    "    (\"ItemB\", 500)\n",
    "], [\"ITEM\", \"SALES\"])\n",
    "\n",
    "# Sort by SALES in ascending order\n",
    "df.sort(\"SALES\", ascending=True).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "640f80a3-860d-40d5-91ba-322f6b0ca13d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Sorting Data in Descending Order\n",
    "\n",
    "**Scenario**: You want to sort the sales data by `Sales` in descending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0546077-4bee-4ba5-847b-b8a237d99061",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemB|  500|\n|ItemC|  400|\n|ItemA|  300|\n|ItemB|  200|\n|ItemA|  100|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort by SALES in descending order\n",
    "df.sort(\"SALES\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "262569b7-dd09-44b7-aba5-c6b59a28daae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Sorting by Multiple Columns\n",
    "\n",
    "**Scenario**: You want to sort the data first by `ITEM` and then by `SALES` in ascending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81301ecf-3efa-4f84-bdcb-d601d956744a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemA|  100|\n|ItemA|  300|\n|ItemB|  200|\n|ItemB|  500|\n|ItemC|  400|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort by ITEM and then by SALES in ascending order\n",
    "df.sort(\"ITEM\", \"SALES\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06f33aa1-2358-4e99-acc6-597106daf986",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Sorting by Multiple Columns with Mixed Orders\n",
    "\n",
    "**Scenario**: You want to sort by `ITEM` in ascending order and `SALES` in descending order.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb75098-1eff-4fa6-b80f-21a81f953613",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemA|  300|\n|ItemA|  100|\n|ItemB|  500|\n|ItemB|  200|\n|ItemC|  400|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort by ITEM in ascending and SALES in descending order\n",
    "df.sort([\"ITEM\", \"SALES\"], ascending=[True, False]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8cd7322-9c93-41f5-9964-94bee251038e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Sorting with Expressions\n",
    "\n",
    "**Scenario**: You want to sort the data based on a calculated column, such as sorting based on sales values after adding 10%.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33261db8-8e87-4b41-8852-de4e7922a821",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemA|  100|\n|ItemB|  200|\n|ItemA|  300|\n|ItemC|  400|\n|ItemB|  500|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Sort by SALES after adding 10% to each value\n",
    "df.sort(expr(\"SALES * 1.1\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e884b974-92e9-4f81-a528-bc9c91483d34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 6. Handling Null Values in Sorting\n",
    "\n",
    "**Scenario**: You want to sort data while handling null values in the `SALES` column, ensuring nulls are placed either first or last.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74914559-506b-4d7a-89e4-61dd6d473688",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemB| null|\n|ItemB| null|\n|ItemA|  100|\n|ItemA|  300|\n|ItemC|  400|\n+-----+-----+\n\n+-----+-----+\n| ITEM|SALES|\n+-----+-----+\n|ItemA|  100|\n|ItemA|  300|\n|ItemC|  400|\n|ItemB| null|\n|ItemB| null|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_with_nulls = spark.createDataFrame([\n",
    "    (\"ItemA\", 100),\n",
    "    (\"ItemB\", None),\n",
    "    (\"ItemA\", 300),\n",
    "    (\"ItemC\", 400),\n",
    "    (\"ItemB\", None)\n",
    "], [\"ITEM\", \"SALES\"])\n",
    "\n",
    "# Sort by SALES and place nulls first\n",
    "df_with_nulls.sort(df_with_nulls.SALES.asc_nulls_first()).show()\n",
    "\n",
    "# Sort by SALES and place nulls last\n",
    "df_with_nulls.sort(df_with_nulls.SALES.asc_nulls_last()).show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "15_pyspark_sort_function_sorting_df_made_easy",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
